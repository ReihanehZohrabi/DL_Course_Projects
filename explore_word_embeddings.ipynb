{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c69aa2e8-ff92-49b2-b63b-015334b598d4",
      "metadata": {
        "id": "c69aa2e8-ff92-49b2-b63b-015334b598d4"
      },
      "source": [
        "<h1 style=\"text-align: center\">\n",
        "In the name of God\n",
        "</h1>\n",
        "<h1 style=\"text-align: center\">\n",
        "Deep Learning HW4\n",
        "</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c61b007d-b6f2-4e44-ad9d-b17a2eb422fe",
      "metadata": {
        "id": "c61b007d-b6f2-4e44-ad9d-b17a2eb422fe"
      },
      "source": [
        "## Imports "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ivKz9YEp0y0G",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivKz9YEp0y0G",
        "outputId": "e4ecb747-2f79-4f2d-b018-3623e764512b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6gdh9TNTBYdp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gdh9TNTBYdp",
        "outputId": "ff13e087-528d-4bae-9dc5-ada2920c5398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/MSC1401_1/DeepLearning/HW4\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/MSC1401_1/DeepLearning/HW4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "49a7e39c-95dc-4811-b325-36243a4e6694",
      "metadata": {
        "id": "49a7e39c-95dc-4811-b325-36243a4e6694"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import math\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataloader import default_collate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9adecc40-4a13-42b3-b2df-4475909a8532",
      "metadata": {
        "id": "9adecc40-4a13-42b3-b2df-4475909a8532"
      },
      "source": [
        "## Some useful constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "20d92b82-4267-4cd7-8a5f-67d7e0e0faad",
      "metadata": {
        "id": "20d92b82-4267-4cd7-8a5f-67d7e0e0faad"
      },
      "outputs": [],
      "source": [
        "EMB_SIZE = 300\n",
        "PAD_TOKEN = '<PAD>'\n",
        "UNK_TOKEN = '<UNK>'\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "TEST_EVAL_BATCH_SIZE = 32\n",
        "# You can define your own constant in here\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2697fea-9c12-4ab7-bb4f-68881db3b471",
      "metadata": {
        "id": "c2697fea-9c12-4ab7-bb4f-68881db3b471"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "160b7647-70ce-4c3f-a1e9-9311eb68c637",
      "metadata": {
        "id": "160b7647-70ce-4c3f-a1e9-9311eb68c637"
      },
      "source": [
        "This data is based on\n",
        "<a href=\"http://www.cs.cornell.edu/people/pabo/movie-review-data/\">this link</a>\n",
        "and contains movie reviews sentiment-analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7131e553-3b17-4cef-b940-7b79f0b45a37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7131e553-3b17-4cef-b940-7b79f0b45a37",
        "outputId": "07719523-6197-4387-bbf0-f58be5f45a1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train lenght is: 8000\n",
            "eval lenght is: 2000\n",
            "test lenght is: 662\n"
          ]
        }
      ],
      "source": [
        "with open('./dataset.json') as f:\n",
        "    all_dataset = json.load(f)\n",
        "    \n",
        "for section in all_dataset.keys():\n",
        "    l = len(all_dataset[section])\n",
        "    print(f\"{section} lenght is: {l}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "226ae1ee-8c58-4701-99ef-344a0a4c6d97",
      "metadata": {
        "id": "226ae1ee-8c58-4701-99ef-344a0a4c6d97"
      },
      "source": [
        "## Download and extract the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de951110-ed5c-4983-84f9-7986a864b870",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de951110-ed5c-4983-84f9-7986a864b870",
        "outputId": "268ccc4e-ca6e-4f58-bbb9-a2fd2d815e1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-01-01 18:11:05--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-01-01 18:11:05--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2023-01-01 18:13:44 (5.19 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d20001-b797-4fbd-ae78-b0e53c5456f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2d20001-b797-4fbd-ae78-b0e53c5456f0",
        "outputId": "33cb4b2e-8c9c-4b68-cc0d-b0b847845489"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  ./glove.6B.zip\n",
            "replace ./glove/glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace ./glove/glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace ./glove/glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace ./glove/glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "!unzip ./glove.6B.zip -d \"./glove/\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "695db153-b2e6-4650-a0f1-f46e62995061",
      "metadata": {
        "id": "695db153-b2e6-4650-a0f1-f46e62995061"
      },
      "source": [
        "## Create embedding matrix and useful functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4d14e34c-032d-4ba0-b55c-e3abb84194a7",
      "metadata": {
        "id": "4d14e34c-032d-4ba0-b55c-e3abb84194a7"
      },
      "outputs": [],
      "source": [
        "word_list = []\n",
        "emb_list = []\n",
        "with open(f'./glove/glove.6B.{EMB_SIZE}d.txt','r') as f:\n",
        "    for line in f.read().strip().split('\\n'):\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        emb = values[1:]\n",
        "        word_list.append(word)\n",
        "        emb_list.append(emb)\n",
        "        \n",
        "emb_matrix = np.array(emb_list, 'float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "acf153cb-e1f6-4e51-8e4d-c3380f145a39",
      "metadata": {
        "id": "acf153cb-e1f6-4e51-8e4d-c3380f145a39"
      },
      "outputs": [],
      "source": [
        "# We initialize <UNK> token as an average of all embedings\n",
        "unk_emb = np.mean(emb_matrix, axis=0, keepdims=True)\n",
        "word_list.append(UNK_TOKEN)\n",
        "emb_matrix = np.vstack((emb_matrix, unk_emb))\n",
        "\n",
        "# We initialize <PAD> token as zeroes\n",
        "pad_emb = np.zeros((1, EMB_SIZE))\n",
        "word_list.append(PAD_TOKEN)\n",
        "emb_matrix = np.vstack((emb_matrix, pad_emb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "28be1a5a-1f6a-4185-9479-ff687013baf8",
      "metadata": {
        "id": "28be1a5a-1f6a-4185-9479-ff687013baf8"
      },
      "outputs": [],
      "source": [
        "reverse_map = {word: id for (id, word) in enumerate(word_list)}\n",
        "\n",
        "def word_to_ids(word: str) -> list:\n",
        "    word = word.strip()\n",
        "    if word == \"\":\n",
        "        return []\n",
        "    if word in reverse_map:\n",
        "        return [reverse_map[word]]\n",
        "    elif word[-3:] in [\"n't\", \"'re\"]:\n",
        "        return word_to_ids(word[:-3]) + word_to_ids(word[-3:])\n",
        "    elif word[-2:] in [\"'s\", \"'d\", \"'m\"]:\n",
        "        return word_to_ids(word[:-2]) + word_to_ids(word[-2:])\n",
        "    else:\n",
        "        word = word.replace(\"'\", \"\")\n",
        "        if word in reverse_map:\n",
        "            return [reverse_map[word]]\n",
        "    return [reverse_map[UNK_TOKEN]]\n",
        "    \n",
        "def id_to_word(id: int) -> str:\n",
        "    return word_list[id]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11c3f21c-b625-4461-b307-9dcfb14bd163",
      "metadata": {
        "id": "11c3f21c-b625-4461-b307-9dcfb14bd163"
      },
      "source": [
        "## Tokenizer and sentence useful tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "27aeef5d-8ef8-4bc9-8514-9447dab88e3c",
      "metadata": {
        "id": "27aeef5d-8ef8-4bc9-8514-9447dab88e3c"
      },
      "outputs": [],
      "source": [
        "def tokenizer(sentence: str) -> list:\n",
        "    sentence = sentence.strip()\n",
        "    return re.split(\"[ -]+\", sentence)\n",
        "\n",
        "def sentence_to_ids(sentence: str) -> list:\n",
        "    return sum(map(word_to_ids, tokenizer(sentence)), [])\n",
        "\n",
        "def ids_to_sentence(ids: list) -> list:\n",
        "    return ' '.join(map(id_to_word, ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8198c170-8349-4f9b-a79a-519a63ccaa61",
      "metadata": {
        "id": "8198c170-8349-4f9b-a79a-519a63ccaa61"
      },
      "source": [
        "# Part 1: Predict relationships among words (30% grade)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a289a5a-a2ba-46ff-a97b-f24948fe2bc6",
      "metadata": {
        "id": "6a289a5a-a2ba-46ff-a97b-f24948fe2bc6"
      },
      "source": [
        "Now you will write a function that will use the word embeddings to predict relationships among words.\n",
        "* The function will take as input three words.\n",
        "* The first two are related to each other.\n",
        "* It will predict a 4th word which is related to the third word in a similar manner as the two first words are related to each other.\n",
        "* As an example, \"Athens is to Greece as Bangkok is to ______\"?\n",
        "* You will write a program that is capable of finding the fourth word.\n",
        "* We will give you a hint to show you how to compute this.\n",
        "\n",
        "A similar analogy would be the following:\n",
        "\n",
        "<img src = 'https://msadraeij.ir:2083/public_assets/vectors.jpg' width=\"width\" height=\"height\" style=\"width:467px;height:200px;\"/>\n",
        "\n",
        "You will implement a function that can tell you the capital of a country.\n",
        "You should use the same methodology shown in the figure above. To do this,\n",
        "compute you'll first compute cosine similarity metric or the Euclidean distance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6c25731-2a8e-4b58-b577-07755848df27",
      "metadata": {
        "id": "b6c25731-2a8e-4b58-b577-07755848df27"
      },
      "source": [
        "### Cosine Similarity\n",
        "\n",
        "The cosine similarity function is:\n",
        "\n",
        "$$\\cos (\\theta)=\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}\\tag{1}$$\n",
        "\n",
        "$A$ and $B$ represent the word vectors and $A_i$ or $B_i$ represent index i of that vector.\n",
        "& Note that if A and B are identical, you will get $cos(\\theta) = 1$.\n",
        "* Otherwise, if they are the total opposite, meaning, $A= -B$, then you would get $cos(\\theta) = -1$.\n",
        "* If you get $cos(\\theta) =0$, that means that they are orthogonal (or perpendicular).\n",
        "* Numbers between 0 and 1 indicate a similarity score.\n",
        "* Numbers between -1-0 indicate a dissimilarity score.\n",
        "\n",
        "**Instructions**: Implement a function that takes in two word vectors and computes the cosine distance.\n",
        "\n",
        "**Hint**: You can use numpy functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccf39e2e-4f49-491c-9f46-d10309106b93",
      "metadata": {
        "id": "ccf39e2e-4f49-491c-9f46-d10309106b93"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(A, B):\n",
        "    A = A.flatten()\n",
        "    B = B.flatten()\n",
        "    #____________YOUR CODE GOES HERE____________\n",
        "    #REPLACE INSTANCES OF 'None' with your code\n",
        "    dot = np.dot(A,B)\n",
        "    norma = np.sqrt(np.dot(A,A))\n",
        "    normb = np.sqrt(np.dot(B,B))\n",
        "    cos = dot/(norma*normb)\n",
        "    #______________END BLOCK CODE_______________\n",
        "    return cos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33ab06d2-9a2c-4ade-b2e3-0439f7f55c9c",
      "metadata": {
        "id": "33ab06d2-9a2c-4ade-b2e3-0439f7f55c9c"
      },
      "source": [
        "### Euclidean distance\n",
        "\n",
        "You will now implement a function that computes the similarity between two vectors using the Euclidean distance.\n",
        "Euclidean distance is defined as:\n",
        "\n",
        "$$ \\begin{aligned} d(\\mathbf{A}, \\mathbf{B})=d(\\mathbf{B}, \\mathbf{A}) &=\\sqrt{\\left(A_{1}-B_{1}\\right)^{2}+\\left(A_{2}-B_{2}\\right)^{2}+\\cdots+\\left(A_{n}-B_{n}\\right)^{2}} \\\\ &=\\sqrt{\\sum_{i=1}^{n}\\left(A_{i}-B_{i}\\right)^{2}} \\end{aligned}$$\n",
        "\n",
        "* $n$ is the number of elements in the vector\n",
        "* $A$ and $B$ are the corresponding word vectors. \n",
        "* The more similar the words, the more likely the Euclidean distance will be close to 0. \n",
        "\n",
        "**Instructions**: Write a function that computes the Euclidean distance between two vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a251e69e-c488-4c6c-9126-c8742841673c",
      "metadata": {
        "id": "a251e69e-c488-4c6c-9126-c8742841673c"
      },
      "outputs": [],
      "source": [
        "def euclidean(A, B):\n",
        "    #____________YOUR CODE GOES HERE____________\n",
        "    d = np.linalg.norm(A-B)\n",
        "    #______________END BLOCK CODE_______________\n",
        "    return d"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f013dc7-8d8a-41e7-8b97-0ce58357974d",
      "metadata": {
        "id": "2f013dc7-8d8a-41e7-8b97-0ce58357974d"
      },
      "source": [
        "### Let's find out queen\n",
        "\n",
        "We are going to find queen using `Cosine Similarity`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bce3ff9-03a4-43c3-9f2b-1c4e0e41c5f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bce3ff9-03a4-43c3-9f2b-1c4e0e41c5f1",
        "outputId": "dd2aa9d2-bd6f-4428-e0d5-88abc1b086d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('king', 0.8065858072579271), ('queen', 0.6896163179535121), ('monarch', 0.5575490920610626), ('throne', 0.5565374575062252), ('princess', 0.5518684267067457)]\n"
          ]
        }
      ],
      "source": [
        "king_emb = emb_matrix[word_to_ids('king')]\n",
        "man_emb = emb_matrix[word_to_ids('man')]\n",
        "woman_emb = emb_matrix[word_to_ids('woman')]\n",
        "\n",
        "query = king_emb - man_emb + woman_emb\n",
        "word_scores = list(map(lambda x: (x[0], cosine_similarity(query, x[1])), zip(word_list[:-1], emb_matrix)))\n",
        "sorted_word_scores = sorted(word_scores, key=lambda x: x[1], reverse=True)\n",
        "top5 = sorted_word_scores[:5]\n",
        "print(top5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f82f0a18-f2e7-4d73-984f-da7f03e2be8d",
      "metadata": {
        "id": "f82f0a18-f2e7-4d73-984f-da7f03e2be8d"
      },
      "source": [
        "**It's Great**\n",
        "\n",
        "Queen is in top5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d51dd2ba-1316-4836-9a68-5c2af5b82c72",
      "metadata": {
        "id": "d51dd2ba-1316-4836-9a68-5c2af5b82c72"
      },
      "source": [
        "### Finding the country of each capital\n",
        "\n",
        "Now, you  will use the previous functions to compute similarities between vectors,\n",
        "and use these to find the capital cities of countries. You will write a function that\n",
        "takes in three words, and the embeddings dictionary. Your task is to find the\n",
        "capital cities. For example, given the following words: \n",
        "\n",
        "- 1: Athens 2: Greece 3: Baghdad,\n",
        "\n",
        "your task is to predict the country 4: Iraq.\n",
        "\n",
        "**Instructions**: \n",
        "\n",
        "1. To predict the capital you might want to look at the *King - Man + Woman = Queen* example above, and implement that scheme into a mathematical function, using the word embeddings and a similarity function.\n",
        "\n",
        "2. Iterate over the embeddings dictionary and compute the cosine similarity score between your vector and the current word embedding.\n",
        "\n",
        "3. You should add a check to make sure that the word you return is not any of the words that you fed into your function. Return the one with the highest score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fcbf002-6d40-43d4-945d-08f0bbeccff0",
      "metadata": {
        "id": "8fcbf002-6d40-43d4-945d-08f0bbeccff0"
      },
      "outputs": [],
      "source": [
        "def get_top5_country(city1, country1, city2):\n",
        "    #____________YOUR CODE GOES HERE____________\n",
        "\n",
        "    group = set((city1, country1, city2))\n",
        "\n",
        "    city1_emb =  emb_matrix[word_to_ids(city1)]\n",
        "    country1_emb = emb_matrix[word_to_ids(country1)]\n",
        "    city2_emb = emb_matrix[word_to_ids(city2)]\n",
        "\n",
        "    query = country1_emb - city1_emb + city2_emb\n",
        "\n",
        "    word_scores = list(filter(lambda x: x[0] not in group,map(lambda x: (x[0], cosine_similarity(query, x[1])), zip(word_list[:-1], emb_matrix))))\n",
        "    sorted_word_scores = sorted(word_scores, key=lambda x: x[1], reverse=True)\n",
        "    top5 = sorted_word_scores[:5]\n",
        "   \n",
        "    #______________END BLOCK CODE_______________\n",
        "    return top5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a91851-93db-4b1b-a2da-8f4e30cda967",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4a91851-93db-4b1b-a2da-8f4e30cda967",
        "outputId": "455d3c7b-75be-4cb3-e680-3894a829d959"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('iran', 0.7953298646285731),\n",
              " ('iranian', 0.6305805062022181),\n",
              " ('syria', 0.5452216125676077),\n",
              " ('ahmadinejad', 0.5421629020046366),\n",
              " ('iranians', 0.5278357585912685)]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_top5_country('athens', 'greece', 'tehran')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55b277ae-bb79-4500-baff-144acc57e422",
      "metadata": {
        "id": "55b277ae-bb79-4500-baff-144acc57e422"
      },
      "source": [
        "**It's Great**\n",
        "\n",
        "Iran is in top5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7845f1f-1670-4a13-a00b-ab4e5fc41597",
      "metadata": {
        "id": "d7845f1f-1670-4a13-a00b-ab4e5fc41597"
      },
      "source": [
        "# Part 2: Plotting the vectors using PCA(20% grade)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25089b5c-b51e-4c72-ada9-a2fb9bc78e57",
      "metadata": {
        "id": "25089b5c-b51e-4c72-ada9-a2fb9bc78e57"
      },
      "source": [
        "Now you will explore the distance between word vectors after reducing their dimension.\n",
        "The technique we will employ is known as\n",
        "[*principal component analysis* (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis).\n",
        "As we saw, we are working in a 300-dimensional space in this case.\n",
        "Although from a computational perspective we were able to perform a good job,\n",
        "it is impossible to visualize results in such high dimensional spaces.\n",
        "\n",
        "You can think of PCA as a method that projects our vectors in a space of reduced\n",
        "dimension, while keeping the maximum information about the original vectors in\n",
        "their reduced counterparts. In this case, by *maximum infomation* we mean that the\n",
        "Euclidean distance between the original vectors and their projected siblings is\n",
        "minimal. Hence vectors that were originally close in the embeddings dictionary,\n",
        "will produce lower dimensional vectors that are still close to each other.\n",
        "\n",
        "You will see that when you map out the words, similar words will be clustered\n",
        "next to each other. For example, the words 'sad', 'happy', 'joyful' all describe\n",
        "emotion and are supposed to be near each other when plotted.\n",
        "The words: 'oil', 'gas', and 'petroleum' all describe natural resources.\n",
        "Words like 'city', 'village', 'town' could be seen as synonyms and describe a\n",
        "similar thing.\n",
        "\n",
        "Before plotting the words, you need to first be able to reduce each word vector\n",
        "with PCA into 2 dimensions and then plot it. The steps to compute PCA are as follows:\n",
        "\n",
        "1. Mean normalize the data\n",
        "2. Compute the covariance matrix of your data ($\\Sigma$). \n",
        "3. Compute the eigenvectors and the eigenvalues of your covariance matrix\n",
        "4. Multiply the first K eigenvectors by your normalized data. The transformation should look something as follows:\n",
        "\n",
        "<img src = 'https://msadraeij.ir:2083/public_assets/word_embf.jpg' width=\"width\" height=\"height\" style=\"width:800px;height:200px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11ea4190-7b4f-44b7-a870-8da4b2afe78f",
      "metadata": {
        "id": "11ea4190-7b4f-44b7-a870-8da4b2afe78f"
      },
      "source": [
        "**Instructions**: \n",
        "\n",
        "You will write a program that takes in a data set where each row corresponds to a word vector. \n",
        "* The word vectors are of dimension 300. \n",
        "* Use PCA to change the 300 dimensions to `n_components` dimensions. \n",
        "* The new matrix should be of dimension `m, n_componentns`. \n",
        "\n",
        "* First de-mean the data\n",
        "* Get the eigenvalues using `linalg.eigh`.  Use `eigh` rather than `eig` since R is symmetric.  The performance gain when using `eigh` instead of `eig` is substantial.\n",
        "* Sort the eigenvectors and eigenvalues by decreasing order of the eigenvalues.\n",
        "* Get a subset of the eigenvectors (choose how many principle components you want to use using `n_components`).\n",
        "* Return the new transformation of the data by multiplying the eigenvectors with the original data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56f9ee08-af39-4be7-8ee7-19fec2cae5d0",
      "metadata": {
        "id": "56f9ee08-af39-4be7-8ee7-19fec2cae5d0"
      },
      "outputs": [],
      "source": [
        "def compute_pca(X, n_components=2):\n",
        "\n",
        "    #____________YOUR CODE GOES HERE____________\n",
        "\n",
        "    X_demeaned = X - np.mean(X, axis=0)\n",
        "    covariance_matrix = np.cov(X_demeaned,rowvar = False)\n",
        "    eigen_vals, eigen_vecs = np.linalg.eigh(covariance_matrix)\n",
        "    idx_sorted = np.argsort(eigen_vals)\n",
        "    idx_sorted_decreasing = idx_sorted[::-1]\n",
        "    eigen_vals_sorted = eigen_vals[idx_sorted_decreasing]\n",
        "    eigen_vecs_sorted = eigen_vecs[:,idx_sorted_decreasing]\n",
        "    eigen_vecs_subset = eigen_vecs_sorted[:,:n_components]\n",
        "    X_reduced = np.dot(eigen_vecs_subset.T,X_demeaned.T).T\n",
        "    #______________END BLOCK CODE_______________\n",
        "\n",
        "    return X_reduced"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8671dc5-a3ee-4d33-bf6f-16797010caf6",
      "metadata": {
        "id": "c8671dc5-a3ee-4d33-bf6f-16797010caf6"
      },
      "source": [
        "### Plot some words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "416eacd4-9748-4f26-9a10-ba92d2350251",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "416eacd4-9748-4f26-9a10-ba92d2350251",
        "outputId": "6909febc-31eb-46a2-eab0-1270ada1a37c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "selected_words = ['oil', 'gas', 'happy', 'sad', 'city', 'town', 'village', 'country', 'continent', 'petroleum', 'joyful']\n",
        "\n",
        "selected_words_emb = np.vstack([emb_matrix[word_to_ids(word)] for word in selected_words])\n",
        "\n",
        "result = compute_pca(selected_words_emb, 2)\n",
        "plt.scatter(result[:, 0], result[:, 1])\n",
        "for i, word in enumerate(selected_words):\n",
        "    plt.annotate(word, xy=(result[i, 0] - 0.05, result[i, 1] + 0.1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9ea1201-fd2d-4439-8e70-050dc8bb03d2",
      "metadata": {
        "id": "f9ea1201-fd2d-4439-8e70-050dc8bb03d2"
      },
      "source": [
        "# Part 3: Classification using a simple bag of words (10% grade)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fc87523-1e14-4747-abdc-9f827c60150e",
      "metadata": {
        "id": "1fc87523-1e14-4747-abdc-9f827c60150e"
      },
      "outputs": [],
      "source": [
        "def sentence_to_emb(sentence: str):\n",
        "    return_value = np.zeros((EMB_SIZE,))\n",
        "    token_ids = sentence_to_ids(sentence)\n",
        "    #____________YOUR CODE GOES HERE____________\n",
        "    return_value = np.mean([emb_matrix[id] for id in token_ids], axis = 0)\n",
        "    #______________END BLOCK CODE_______________\n",
        "    return return_value"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93d63627-0fbf-4012-be7e-07302a596702",
      "metadata": {
        "id": "93d63627-0fbf-4012-be7e-07302a596702"
      },
      "source": [
        "### Define our simple model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbde2f88-0a2e-43bf-a8a2-e21bd3c22669",
      "metadata": {
        "id": "dbde2f88-0a2e-43bf-a8a2-e21bd3c22669"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(EMB_SIZE, EMB_SIZE),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(EMB_SIZE, EMB_SIZE),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(EMB_SIZE, EMB_SIZE),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(EMB_SIZE, EMB_SIZE),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(EMB_SIZE, 2)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d23a82be-2560-4dd7-a574-0f5fdcd93105",
      "metadata": {
        "id": "d23a82be-2560-4dd7-a574-0f5fdcd93105"
      },
      "source": [
        "### Define our dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "799a7e9d-9216-48fe-b3ff-dda296796fc9",
      "metadata": {
        "id": "799a7e9d-9216-48fe-b3ff-dda296796fc9"
      },
      "outputs": [],
      "source": [
        "class Method1Dataset(Dataset):\n",
        "    def __init__(self, datadict):\n",
        "        self.data = [(sentence_to_emb(sentence), semantic)for sentence, semantic in datadict]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c908988-8291-4b67-a7bb-499065bf590f",
      "metadata": {
        "id": "2c908988-8291-4b67-a7bb-499065bf590f"
      },
      "outputs": [],
      "source": [
        "train_dataset = Method1Dataset(all_dataset['train'])\n",
        "eval_dataset = Method1Dataset(all_dataset['eval'])\n",
        "test_dataset = Method1Dataset(all_dataset['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a215fcf5-ff3e-4fe6-9029-f365cd5e286e",
      "metadata": {
        "id": "a215fcf5-ff3e-4fe6-9029-f365cd5e286e"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=TEST_EVAL_BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=TEST_EVAL_BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7647d0d4-081e-4829-a606-dd9d37e1a3a5",
      "metadata": {
        "id": "7647d0d4-081e-4829-a606-dd9d37e1a3a5"
      },
      "source": [
        "### Define our training routine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b189554c-b897-4fef-9bc5-c22aa2e82dd9",
      "metadata": {
        "id": "b189554c-b897-4fef-9bc5-c22aa2e82dd9"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f5a7eb0-a8ba-44e4-84db-ae979b4bbd9a",
      "metadata": {
        "id": "3f5a7eb0-a8ba-44e4-84db-ae979b4bbd9a"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        x, trg = batch\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x.float())\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "558bd8c4-ee8c-49fd-8ec0-07c422147fe4",
      "metadata": {
        "id": "558bd8c4-ee8c-49fd-8ec0-07c422147fe4"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            x, trg = batch\n",
        "            output = model(x.float())\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f0a8e94-db8b-4370-9df0-f2cf5b205027",
      "metadata": {
        "id": "4f0a8e94-db8b-4370-9df0-f2cf5b205027"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa84453b-e125-4fe6-bd3e-1cdf35e66206",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa84453b-e125-4fe6-bd3e-1cdf35e66206",
        "outputId": "7d874205-f062-41d2-a92f-c6d35f494f69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 0.606\n",
            "\t Val. Loss: 0.600\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.522\n",
            "\t Val. Loss: 0.520\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.488\n",
            "\t Val. Loss: 0.489\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.467\n",
            "\t Val. Loss: 0.577\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.450\n",
            "\t Val. Loss: 0.505\n",
            "Epoch: 06\n",
            "\tTrain Loss: 0.431\n",
            "\t Val. Loss: 0.501\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.407\n",
            "\t Val. Loss: 0.512\n",
            "Epoch: 08\n",
            "\tTrain Loss: 0.382\n",
            "\t Val. Loss: 0.522\n",
            "Epoch: 09\n",
            "\tTrain Loss: 0.346\n",
            "\t Val. Loss: 0.539\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.324\n",
            "\t Val. Loss: 0.593\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(10):\n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion)\n",
        "    valid_loss = evaluate(model, eval_dataloader, criterion)\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26fd26e2-ba81-4b48-9915-ebfe69aa7c20",
      "metadata": {
        "id": "26fd26e2-ba81-4b48-9915-ebfe69aa7c20"
      },
      "source": [
        "### Check the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57eddc71-714b-42d6-8a67-2bc98c882046",
      "metadata": {
        "id": "57eddc71-714b-42d6-8a67-2bc98c882046"
      },
      "outputs": [],
      "source": [
        "def get_all_targets_and_predicted(model, iterator):\n",
        "    all_trg = []\n",
        "    all_prd = []\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            x, trg = batch\n",
        "            output = model(x.float())\n",
        "            prd = output.argmax(1).tolist()\n",
        "            \n",
        "            all_trg += trg\n",
        "            all_prd += prd\n",
        "    return all_trg, all_prd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1ed020c-f21d-4d6f-b84b-4f2db54cbf7f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1ed020c-f21d-4d6f-b84b-4f2db54cbf7f",
        "outputId": "cf5abe16-a95e-43a3-c1fe-6e136e654deb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__________________TRAIN DATASET__________________\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.88      0.90      4000\n",
            "           1       0.89      0.92      0.90      4000\n",
            "\n",
            "    accuracy                           0.90      8000\n",
            "   macro avg       0.90      0.90      0.90      8000\n",
            "weighted avg       0.90      0.90      0.90      8000\n",
            "\n",
            "__________________EVAL DATASET__________________\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.71      0.73      1000\n",
            "           1       0.73      0.77      0.75      1000\n",
            "\n",
            "    accuracy                           0.74      2000\n",
            "   macro avg       0.74      0.74      0.74      2000\n",
            "weighted avg       0.74      0.74      0.74      2000\n",
            "\n",
            "__________________TEST DATASET__________________\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.75      0.76       331\n",
            "           1       0.75      0.77      0.76       331\n",
            "\n",
            "    accuracy                           0.76       662\n",
            "   macro avg       0.76      0.76      0.76       662\n",
            "weighted avg       0.76      0.76      0.76       662\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('__________________TRAIN DATASET__________________')\n",
        "trg, prd = get_all_targets_and_predicted(model, train_dataloader)\n",
        "print(classification_report(trg, prd))\n",
        "print('__________________EVAL DATASET__________________')\n",
        "trg, prd = get_all_targets_and_predicted(model, eval_dataloader)\n",
        "print(classification_report(trg, prd))\n",
        "print('__________________TEST DATASET__________________')\n",
        "trg, prd = get_all_targets_and_predicted(model, test_dataloader)\n",
        "print(classification_report(trg, prd))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "134984dd-592a-4f74-84a7-bd11b16a203a",
      "metadata": {
        "id": "134984dd-592a-4f74-84a7-bd11b16a203a"
      },
      "source": [
        "# Part 4: Classification using an LSTM classifier (40% grade)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15c8f6ae-33f0-40c9-b22c-9ffbac4413ee",
      "metadata": {
        "id": "15c8f6ae-33f0-40c9-b22c-9ffbac4413ee"
      },
      "source": [
        "Use pytorch to implement an lstm model classifier. You can use any hyperparameters you want. You must report `classification_report` like previous example for all datasets. Remember your embeding layer initial value must be `emb_matrix`.\n",
        "\n",
        "You must train the model two times.\n",
        "First time freeze the embeding layer and second time fine-tune it end2end.\n",
        "After that compare these two results and explatin the trade-off between freezing or fine-tuning the embedding layer.\n",
        "You can read more about this in <a href=\"https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture04-backprop.pdf\">this link</a>.\n",
        "\n",
        "Good Luck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ooXtHuu-LwiV",
      "metadata": {
        "id": "ooXtHuu-LwiV"
      },
      "outputs": [],
      "source": [
        "#To be able to use more ram :)\n",
        "del word_list, emb_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "Fayh9LCUj5rM",
      "metadata": {
        "id": "Fayh9LCUj5rM"
      },
      "outputs": [],
      "source": [
        "class Method2Dataset(Dataset):\n",
        "    def __init__(self, datadict):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        for sentence, semantic in datadict:\n",
        "            self.data.append(sentence)\n",
        "            self.labels.append(semantic)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx],self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "Q3-bgZvTqcBx",
      "metadata": {
        "id": "Q3-bgZvTqcBx"
      },
      "outputs": [],
      "source": [
        "train_dataset = Method2Dataset(all_dataset['train'])\n",
        "eval_dataset = Method2Dataset(all_dataset['eval'])\n",
        "test_dataset = Method2Dataset(all_dataset['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "u75CbwrDeNfw",
      "metadata": {
        "id": "u75CbwrDeNfw"
      },
      "outputs": [],
      "source": [
        "# For this part I used methods of the following link for converting sentences to tensor as lstm input\n",
        "# https://www.kaggle.com/code/fyycssx/first-try-lstm-with-glove-by-pytorch\n",
        "def sentences_to_tensors(sentences, word_to_ids, max_len = 64, pad_token = 0):\n",
        "    # Returns a tensor of padded indexes of word ids in a sentence\n",
        "    word_ids_list = [sentence_to_ids(sent) for sent in sentences]\n",
        "    max_len = min(max_len,max([len(indexes) for indexes in word_ids_list]))\n",
        "    truncated_indexes = [indexes[:max_len] for indexes in word_ids_list]\n",
        "    padded_indexes = [indexes+[0]*(max_len - len(indexes)) for indexes in truncated_indexes]\n",
        "    return torch.LongTensor(padded_indexes)\n",
        "\n",
        "def train_collate(batch):\n",
        "    data,labels = zip(*batch)\n",
        "    input_tensor = sentences_to_tensors(data,word_to_ids)\n",
        "    return input_tensor,torch.LongTensor(labels)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset,batch_size= TRAIN_BATCH_SIZE, shuffle = True,collate_fn=train_collate,drop_last = True)\n",
        "val_loader = DataLoader(eval_dataset,batch_size=TEST_EVAL_BATCH_SIZE,shuffle=False,collate_fn=train_collate,drop_last = True)\n",
        "test_loader = DataLoader(test_dataset,batch_size=TEST_EVAL_BATCH_SIZE,shuffle=False,collate_fn=train_collate,drop_last = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "0SkVD-lDe5vS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SkVD-lDe5vS",
        "outputId": "16c77bdd-8f6e-49c8-ba42-132806b44170"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  1402,   3244,      4,  ...,      0,      0,      0],\n",
            "        [    12,   4694,     54,  ...,      0,      0,      0],\n",
            "        [  4022,      1,      7,  ...,      0,      0,      0],\n",
            "        ...,\n",
            "        [  4779,      0,    870,  ...,      0,      0,      0],\n",
            "        [   575, 113113,     29,  ...,      0,      0,      0],\n",
            "        [  7014,   4943,    118,  ..., 106593,      2,      0]])\n",
            "torch.Size([32, 39])\n",
            "torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "for data, labels in train_loader:\n",
        "      print(data)\n",
        "      print(data.shape)\n",
        "      print(labels.shape)\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "0b1ac873-86c1-4276-83e5-287dd0676fb7",
      "metadata": {
        "id": "0b1ac873-86c1-4276-83e5-287dd0676fb7"
      },
      "outputs": [],
      "source": [
        "class SentimentLSTM(nn.Module):\n",
        "    def __init__(self, output_size, hidden_dim, n_layers,emb_matrix, freeze, drop_prob=0.5 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_dim = emb_matrix.shape[1]\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding.from_pretrained(embeddings=torch.Tensor(emb_matrix).to(device), freeze = freeze)\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, hidden_dim, n_layers, bidirectional = True,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim*2, output_size)\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x,(hs,cs) = self.lstm(x)\n",
        "        x = self.fc(x[:,-1,:])\n",
        "        return  x\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "a8sa_RP_6ddl",
      "metadata": {
        "id": "a8sa_RP_6ddl"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        x, trg = batch\n",
        "        x = x.cuda()\n",
        "        trg = trg.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            x, trg = batch\n",
        "            x = x.cuda()\n",
        "            trg = trg.cuda()\n",
        "            output = model(x)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)\n",
        "def get_all_targets_and_predicted(model, iterator):\n",
        "    all_trg = []\n",
        "    all_prd = []\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            x, trg = batch\n",
        "            x = x.cuda()\n",
        "            trg = trg.cuda()\n",
        "            output = model(x)\n",
        "            prd = output.cpu().numpy().argmax(1).tolist()\n",
        "            \n",
        "            all_trg += trg.cpu().numpy().tolist()\n",
        "            all_prd += prd\n",
        "    return all_trg, all_prd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZND6Q2EMvfAs",
      "metadata": {
        "id": "ZND6Q2EMvfAs"
      },
      "source": [
        "#Freezing Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "51dozLbz3jBu",
      "metadata": {
        "id": "51dozLbz3jBu"
      },
      "outputs": [],
      "source": [
        "no_layers = 2\n",
        "output_dim = 2\n",
        "hidden_dim = 128\n",
        "\n",
        "\n",
        "model_freeze = SentimentLSTM(output_dim, hidden_dim, no_layers,emb_matrix,True,drop_prob=0.5)\n",
        "model_freeze.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_freeze.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "f0aEyUfo43mf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0aEyUfo43mf",
        "outputId": "18c99979-9d65-463a-c088-9a45f655d850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 0.695\n",
            "\t Val. Loss: 0.693\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.693\n",
            "\t Val. Loss: 0.693\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.693\n",
            "\t Val. Loss: 0.694\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.693\n",
            "\t Val. Loss: 0.693\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.672\n",
            "\t Val. Loss: 0.572\n",
            "Epoch: 06\n",
            "\tTrain Loss: 0.531\n",
            "\t Val. Loss: 0.494\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.466\n",
            "\t Val. Loss: 0.488\n",
            "Epoch: 08\n",
            "\tTrain Loss: 0.412\n",
            "\t Val. Loss: 0.462\n",
            "Epoch: 09\n",
            "\tTrain Loss: 0.362\n",
            "\t Val. Loss: 0.472\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.315\n",
            "\t Val. Loss: 0.517\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(10):\n",
        "    train_loss = train(model_freeze, train_loader, optimizer, criterion)\n",
        "    valid_loss = evaluate(model_freeze, val_loader, criterion)\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "EvzvZgMJ5RcN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvzvZgMJ5RcN",
        "outputId": "a8d19892-d87e-4ea9-c4f7-3f5209eca4ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__________________TRAIN DATASET__________________\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.94      0.91      4000\n",
            "           1       0.94      0.87      0.90      4000\n",
            "\n",
            "    accuracy                           0.91      8000\n",
            "   macro avg       0.91      0.91      0.91      8000\n",
            "weighted avg       0.91      0.91      0.91      8000\n",
            "\n",
            "__________________EVAL DATASET__________________\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.83      0.79       990\n",
            "           1       0.81      0.73      0.77       994\n",
            "\n",
            "    accuracy                           0.78      1984\n",
            "   macro avg       0.79      0.78      0.78      1984\n",
            "weighted avg       0.79      0.78      0.78      1984\n",
            "\n",
            "__________________TEST DATASET__________________\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.88      0.82       323\n",
            "           1       0.86      0.73      0.79       317\n",
            "\n",
            "    accuracy                           0.80       640\n",
            "   macro avg       0.81      0.80      0.80       640\n",
            "weighted avg       0.81      0.80      0.80       640\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('__________________TRAIN DATASET__________________')\n",
        "trg, prd = get_all_targets_and_predicted(model_freeze, train_loader)\n",
        "print(classification_report(trg, prd))\n",
        "print('__________________EVAL DATASET__________________')\n",
        "trg, prd = get_all_targets_and_predicted(model_freeze, val_loader)\n",
        "print(classification_report(trg, prd))\n",
        "print('__________________TEST DATASET__________________')\n",
        "trg, prd = get_all_targets_and_predicted(model_freeze, test_loader)\n",
        "print(classification_report(trg, prd))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SitA2l-bv9dN",
      "metadata": {
        "id": "SitA2l-bv9dN"
      },
      "source": [
        "#Finetuning Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "MX4rs609wAYJ",
      "metadata": {
        "id": "MX4rs609wAYJ"
      },
      "outputs": [],
      "source": [
        "no_layers = 2\n",
        "output_dim = 2\n",
        "hidden_dim = 256\n",
        "\n",
        "\n",
        "model = SentimentLSTM(output_dim, hidden_dim, no_layers,emb_matrix,False,drop_prob=0.5)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "CbzomPTWwD9E",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbzomPTWwD9E",
        "outputId": "8fd36190-97a0-4957-f065-dc629cbefe0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 0.694\n",
            "\t Val. Loss: 0.694\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.694\n",
            "\t Val. Loss: 0.693\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.693\n",
            "\t Val. Loss: 0.693\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.686\n",
            "\t Val. Loss: 0.618\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.484\n",
            "\t Val. Loss: 0.444\n",
            "Epoch: 06\n",
            "\tTrain Loss: 0.224\n",
            "\t Val. Loss: 0.600\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.102\n",
            "\t Val. Loss: 0.724\n",
            "Epoch: 08\n",
            "\tTrain Loss: 0.052\n",
            "\t Val. Loss: 0.831\n",
            "Epoch: 09\n",
            "\tTrain Loss: 0.027\n",
            "\t Val. Loss: 1.096\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.014\n",
            "\t Val. Loss: 1.077\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(10):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion)\n",
        "    valid_loss = evaluate(model, val_loader, criterion)\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "AKZmaXYRwGiF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKZmaXYRwGiF",
        "outputId": "ca5bee5e-23d9-4d0c-d83f-3b6e7578ea5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__________________TRAIN DATASET__________________\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4000\n",
            "           1       1.00      1.00      1.00      4000\n",
            "\n",
            "    accuracy                           1.00      8000\n",
            "   macro avg       1.00      1.00      1.00      8000\n",
            "weighted avg       1.00      1.00      1.00      8000\n",
            "\n",
            "__________________EVAL DATASET__________________\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.76      0.77       990\n",
            "           1       0.77      0.79      0.78       994\n",
            "\n",
            "    accuracy                           0.77      1984\n",
            "   macro avg       0.77      0.77      0.77      1984\n",
            "weighted avg       0.77      0.77      0.77      1984\n",
            "\n",
            "__________________TEST DATASET__________________\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.79      0.79       323\n",
            "           1       0.78      0.79      0.78       317\n",
            "\n",
            "    accuracy                           0.79       640\n",
            "   macro avg       0.79      0.79      0.79       640\n",
            "weighted avg       0.79      0.79      0.79       640\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('__________________TRAIN DATASET__________________')\n",
        "trg, prd = get_all_targets_and_predicted(model, train_loader)\n",
        "print(classification_report(trg, prd))\n",
        "print('__________________EVAL DATASET__________________')\n",
        "trg, prd = get_all_targets_and_predicted(model, val_loader)\n",
        "print(classification_report(trg, prd))\n",
        "print('__________________TEST DATASET__________________')\n",
        "trg, prd = get_all_targets_and_predicted(model, test_loader)\n",
        "print(classification_report(trg, prd))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "Go0naI_U39uz",
      "metadata": {
        "id": "Go0naI_U39uz"
      },
      "source": [
        "Analysis:\n",
        "The embedding matrix which used in the initialization of the Embedding layer is highly trained on a large corpus of text. The training and the data are so huge that the embedding has learnt a type of association between words.\n",
        "We can enjoy the benefits of such an embedding by keeping it untrainable. As it is obvious from results, additional training in the context of our task, may result in unusual behaviour of the Embedding layer and also distort the learned associations. Also the model may overfit the data."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8198c170-8349-4f9b-a79a-519a63ccaa61",
        "d7845f1f-1670-4a13-a00b-ab4e5fc41597"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python [conda env:root] *",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
